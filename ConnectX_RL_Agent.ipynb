{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147a668d",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent for Connect X\n",
    "\n",
    "In this notebook, we'll build a reinforcement learning agent to play Connect X (a variant of Connect Four). We'll start by understanding the game mechanics, create a baseline agent, and then implement advanced RL algorithms like Deep Q-Network (DQN) and Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1f440477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from kaggle_environments import make\n",
    "from copy import deepcopy\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cbe2d",
   "metadata": {},
   "source": [
    "## Understanding Connect X Environment\n",
    "\n",
    "Connect X is a classic \"connect N in a row\" game similar to Connect Four. The environment has the following characteristics:\n",
    "- Board size: 7 columns Ã— 6 rows (by default)\n",
    "- Goal: Connect 4 pieces in a row (horizontally, vertically, or diagonally)\n",
    "- Players: 2 players (Player 1 and Player 2)\n",
    "- Actions: Drop a piece in any of the 7 columns\n",
    "- Rewards: -1 for losing, 0 for draw/ongoing, +1 for winning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8785b436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configuration:\n",
      "Columns: 7\n",
      "Rows: 6\n",
      "Pieces in a row to win: 4\n",
      "Action space: 7 columns (0 to 6)\n"
     ]
    }
   ],
   "source": [
    "env = make(\"connectx\")\n",
    "print(\"Environment configuration:\")\n",
    "print(f\"Columns: {env.configuration.columns}\")\n",
    "print(f\"Rows: {env.configuration.rows}\")\n",
    "print(f\"Pieces in a row to win: {env.configuration.inarow}\")\n",
    "print(f\"Action space: {env.configuration.columns} columns (0 to {env.configuration.columns-1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a940ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73205172",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's start by creating a simple random agent to establish a baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ca761e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing random agent...\n",
      "Initial board: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Our mark: 1\n"
     ]
    }
   ],
   "source": [
    "def random_agent(obs, config):\n",
    "    \"\"\"\n",
    "    Random agent that selects a valid column randomly.\n",
    "    \n",
    "    Args:\n",
    "        obs: Observation from the environment\n",
    "        config: Configuration of the environment\n",
    "        \n",
    "    Returns:\n",
    "        int: Column index to place the piece\n",
    "    \"\"\"\n",
    "    # Get all valid moves (non-full columns)\n",
    "    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "    return random.choice(valid_moves)\n",
    "\n",
    "# Test the random agent\n",
    "print(\"Testing random agent...\")\n",
    "trainer = env.train([None, \"random\"])  # Play against built-in random agent\n",
    "obs = trainer.reset()\n",
    "print(f\"Initial board: {obs.board}\")\n",
    "print(f\"Our mark: {obs.mark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b74f3a",
   "metadata": {},
   "source": [
    "##  Create  Baseline Agent\n",
    "\n",
    "A slightly more sophisticated agent that implements basic heuristics like blocking opponent wins and completing own wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "34c75158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline agents created!\n"
     ]
    }
   ],
   "source": [
    "def random_agent(obs, config):\n",
    "    \"\"\"Random agent that selects a valid column randomly.\"\"\"\n",
    "    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "    return random.choice(valid_moves)\n",
    "\n",
    "\n",
    "def check_winning_move(obs, config, column, mark):\n",
    "    \"\"\"Check if placing a piece in the given column would result in a win.\"\"\"\n",
    "    board = np.array(obs.board).reshape(config.rows, config.columns)\n",
    "    \n",
    "    # Find the lowest empty row in the column\n",
    "    row = 0\n",
    "    for r in range(config.rows-1, -1, -1):\n",
    "        if board[r][column] == 0:\n",
    "            row = r\n",
    "            break\n",
    "    \n",
    "    # Place the temporary piece\n",
    "    board[row][column] = mark\n",
    "    \n",
    "    # Check for horizontal win\n",
    "    inarow = config.inarow - 1\n",
    "    count = 0\n",
    "    for c in range(max(0, column-inarow), min(config.columns, column+inarow+1)):\n",
    "        if board[row][c] == mark:\n",
    "            count += 1\n",
    "            if count == config.inarow:\n",
    "                return True\n",
    "        else:\n",
    "            count = 0\n",
    "    \n",
    "    # Check for vertical win\n",
    "    count = 0\n",
    "    for r in range(max(0, row-inarow), min(config.rows, row+inarow+1)):\n",
    "        if board[r][column] == mark:\n",
    "            count += 1\n",
    "            if count == config.inarow:\n",
    "                return True\n",
    "        else:\n",
    "            count = 0\n",
    "    \n",
    "    # Check for diagonal wins\n",
    "    count = 0\n",
    "    for i in range(-inarow, inarow+1):\n",
    "        r, c = row+i, column+i\n",
    "        if 0 <= r < config.rows and 0 <= c < config.columns and board[r][c] == mark:\n",
    "            count += 1\n",
    "            if count == config.inarow:\n",
    "                return True\n",
    "        else:\n",
    "            count = 0\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(-inarow, inarow+1):\n",
    "        r, c = row+i, column-i\n",
    "        if 0 <= r < config.rows and 0 <= c < config.columns and board[r][c] == mark:\n",
    "            count += 1\n",
    "            if count == config.inarow:\n",
    "                return True\n",
    "        else:\n",
    "            count = 0\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def rule_based_agent(obs, config):\n",
    "    \"\"\"\n",
    "    Rule-based agent with heuristics:\n",
    "    1. Play winning move if available\n",
    "    2. Block opponent's winning move\n",
    "    3. Prefer center column\n",
    "    4. Otherwise play randomly\n",
    "    \"\"\"\n",
    "    valid_moves = [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "    \n",
    "    # Check for winning moves\n",
    "    for col in valid_moves:\n",
    "        if check_winning_move(obs, config, col, obs.mark):\n",
    "            return col\n",
    "    \n",
    "    # Check for blocking opponent's winning moves\n",
    "    opponent_mark = 3 - obs.mark\n",
    "    for col in valid_moves:\n",
    "        if check_winning_move(obs, config, col, opponent_mark):\n",
    "            return col\n",
    "    \n",
    "    # Prefer center column\n",
    "    center_col = config.columns // 2\n",
    "    if center_col in valid_moves:\n",
    "        return center_col\n",
    "    \n",
    "    return random.choice(valid_moves)\n",
    "\n",
    "print(\"Baseline agents created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fa46a",
   "metadata": {},
   "source": [
    "## State Encoding and Helper Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bc363c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def encode_board(obs, config):\n",
    "    \"\"\"\n",
    "    Encode board state into neural network input format.\n",
    "    Creates 3 channels: our pieces, opponent pieces, empty spaces.\n",
    "    \"\"\"\n",
    "    board = np.array(obs.board).reshape(config.rows, config.columns)\n",
    "    \n",
    "    my_pieces = np.where(board == obs.mark, 1, 0)\n",
    "    opp_pieces = np.where(board == (3 - obs.mark), 1, 0)\n",
    "    empty_spaces = np.where(board == 0, 1, 0)\n",
    "    \n",
    "    state = np.stack([my_pieces, opp_pieces, empty_spaces]).flatten()\n",
    "    return state.astype(np.float32)\n",
    "\n",
    "\n",
    "def get_valid_actions(obs, config):\n",
    "    \"\"\"Get list of valid actions (columns that are not full).\"\"\"\n",
    "    return [col for col in range(config.columns) if obs.board[col] == 0]\n",
    "\n",
    "print(\"Helper functions defined!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb5a1d",
   "metadata": {},
   "source": [
    "## : DQN Neural Network\n",
    "\n",
    "Now we'll implement a DQN agent which is a foundational deep reinforcement learning algorithm. DQN learns a Q-function that estimates the expected future rewards for taking each action in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "88452f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN network architecture defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "print(\"DQN network architecture defined!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938e08d",
   "metadata": {},
   "source": [
    "# CELL 6: Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d2719d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer defined!\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.buffer.append((state, action, next_state, reward))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"Replay buffer defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99528e32",
   "metadata": {},
   "source": [
    "#  FIXED DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b7a90b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXED DQN Agent defined!\n"
     ]
    }
   ],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_size=128, lr=0.001):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Neural networks\n",
    "        self.q_network = DQN(state_size, hidden_size, action_size).to(self.device)\n",
    "        self.target_network = DQN(state_size, hidden_size, action_size).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Training parameters\n",
    "        self.memory = ReplayBuffer(10000)\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.update_target_freq = 100  # FIXED: Update every 100 learning steps\n",
    "        self.learn_step = 0\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        # Learn every 4 steps\n",
    "        self.t_step = (self.t_step + 1) % 4\n",
    "        if self.t_step == 0 and len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample(self.batch_size)\n",
    "            self.learn(experiences)\n",
    "    \n",
    "    def act(self, state, valid_actions, eps=None):\n",
    "        \"\"\"FIXED: Returns actions with proper action masking.\"\"\"\n",
    "        if eps is None:\n",
    "            eps = self.epsilon\n",
    "            \n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        self.q_network.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.q_network(state)\n",
    "        self.q_network.train()\n",
    "        \n",
    "        # FIXED: Epsilon-greedy with action masking\n",
    "        if random.random() > eps:\n",
    "            # Mask invalid actions with -inf\n",
    "            action_values_np = action_values.cpu().data.numpy()[0]\n",
    "            masked_values = np.full(self.action_size, -np.inf)\n",
    "            for action in valid_actions:\n",
    "                masked_values[action] = action_values_np[action]\n",
    "            return np.argmax(masked_values)\n",
    "        else:\n",
    "            return random.choice(valid_actions)\n",
    "    \n",
    "    def learn(self, experiences):\n",
    "        \"\"\"FIXED: Update with gradient clipping.\"\"\"\n",
    "        states = torch.FloatTensor(np.array([e[0] for e in experiences])).to(self.device)\n",
    "        actions = torch.LongTensor([e[1] for e in experiences]).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array([e[2] for e in experiences])).to(self.device)\n",
    "        rewards = torch.FloatTensor([e[3] for e in experiences]).to(self.device)\n",
    "        \n",
    "        # Get max predicted Q values from target model\n",
    "        Q_targets_next = self.target_network(next_states).detach().max(1)[0]\n",
    "        \n",
    "        # Compute Q targets\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next)\n",
    "        \n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)  # ADDED: Gradient clipping\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # FIXED: Update target network at proper frequency\n",
    "        self.learn_step += 1\n",
    "        if self.learn_step % self.update_target_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Decrease epsilon for more exploitation over time.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"FIXED DQN Agent defined!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f97da3",
   "metadata": {},
   "source": [
    "#  FIXED DQN Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ba3ddc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXED DQN training function defined!\n"
     ]
    }
   ],
   "source": [
    "def train_dqn_agent(episodes=1000, opponent='random', epsilon_start=1.0):\n",
    "    \"\"\"\n",
    "    FIXED: Train DQN agent with proper reward tracking and shaping.\n",
    "    \"\"\"\n",
    "    env = make(\"connectx\")\n",
    "    \n",
    "    state_size = env.configuration.columns * env.configuration.rows * 3\n",
    "    action_size = env.configuration.columns\n",
    "    \n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    agent.epsilon = epsilon_start\n",
    "    \n",
    "    # FIXED: Track wins and rewards properly\n",
    "    wins_window = deque(maxlen=100)\n",
    "    episode_rewards = deque(maxlen=100)\n",
    "    \n",
    "    print(\"Training DQN agent...\")\n",
    "    for episode in range(episodes):\n",
    "        # Set up opponent\n",
    "        if opponent == 'random':\n",
    "            trainer = env.train([None, random_agent])\n",
    "        elif opponent == 'rule_based':\n",
    "            trainer = env.train([None, rule_based_agent])\n",
    "        else:\n",
    "            trainer = env.train([None, opponent])\n",
    "            \n",
    "        obs = trainer.reset()\n",
    "        state = encode_board(obs, env.configuration)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Get valid actions\n",
    "            valid_actions = get_valid_actions(obs, env.configuration)\n",
    "            \n",
    "            # Select action\n",
    "            action = agent.act(state, valid_actions)\n",
    "            \n",
    "            # Take action\n",
    "            next_obs, reward, done, info = trainer.step(action)\n",
    "            \n",
    "            # FIXED: Better reward shaping\n",
    "            if done:\n",
    "                if reward == 1:  # Win\n",
    "                    shaped_reward = 1.0\n",
    "                    wins_window.append(1)\n",
    "                elif reward == -1:  # Loss\n",
    "                    shaped_reward = -1.0\n",
    "                    wins_window.append(0)\n",
    "                else:  # Draw\n",
    "                    shaped_reward = 0.5\n",
    "                    wins_window.append(0)\n",
    "            else:\n",
    "                shaped_reward = 0.0  # No step penalty\n",
    "                next_state = encode_board(next_obs, env.configuration)\n",
    "            \n",
    "            # Store experience\n",
    "            if done:\n",
    "                next_state = state  # Terminal state\n",
    "            \n",
    "            agent.step(state, action, shaped_reward, next_state, done)\n",
    "            \n",
    "            episode_reward += shaped_reward\n",
    "            \n",
    "            # Move to next state\n",
    "            if not done:\n",
    "                state = next_state\n",
    "                obs = next_obs\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Update epsilon\n",
    "        agent.update_epsilon()\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            win_rate = np.mean(wins_window) * 100 if wins_window else 0\n",
    "            avg_reward = np.mean(episode_rewards) if episode_rewards else 0\n",
    "            print(f'Episode {episode+1}/{episodes} | Win Rate: {win_rate:.1f}% | '\n",
    "                  f'Avg Reward: {avg_reward:.3f} | Epsilon: {agent.epsilon:.3f}')\n",
    "    \n",
    "    final_win_rate = np.mean(wins_window) * 100 if wins_window else 0\n",
    "    print(f'Training completed! Final win rate: {final_win_rate:.1f}%')\n",
    "    return agent\n",
    "\n",
    "print(\"FIXED DQN training function defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdcabd6",
   "metadata": {},
   "source": [
    "#  FIXED PPO Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6f030073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXED Actor-Critic network defined!\n"
     ]
    }
   ],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared_fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.shared_fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Actor head (policy network)\n",
    "        self.actor_fc = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.actor_out = nn.Linear(hidden_size // 2, action_size)\n",
    "        \n",
    "        # Critic head (value network)\n",
    "        self.critic_fc = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.critic_out = nn.Linear(hidden_size // 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.shared_fc1(x))\n",
    "        x = F.relu(self.shared_fc2(x))\n",
    "        \n",
    "        # Actor (policy)\n",
    "        actor_x = F.relu(self.actor_fc(x))\n",
    "        action_probs = F.softmax(self.actor_out(actor_x), dim=-1)\n",
    "        \n",
    "        # FIXED: Critic output - squeeze to 1D\n",
    "        critic_x = F.relu(self.critic_fc(x))\n",
    "        state_value = self.critic_out(critic_x).squeeze(-1)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "\n",
    "print(\"FIXED Actor-Critic network defined!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbdf2b",
   "metadata": {},
   "source": [
    "# PPO Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f779195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Memory defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "print(\"PPO Memory defined!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5b62c",
   "metadata": {},
   "source": [
    "#  FIXED PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f1e797ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXED PPO Agent defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_size, action_size, lr=0.0003, gamma=0.99, eps_clip=0.2, K_epochs=4):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.policy_old = ActorCritic(state_size, action_size).to(self.device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def act(self, state, valid_actions):\n",
    "        \"\"\"Select action using current policy.\"\"\"\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_probs, state_val = self.policy_old(state)\n",
    "            \n",
    "            # Mask invalid actions\n",
    "            masked_probs = torch.zeros_like(action_probs)\n",
    "            for action in valid_actions:\n",
    "                masked_probs[0][action] = action_probs[0][action]\n",
    "            \n",
    "            # Renormalize\n",
    "            masked_probs = masked_probs / (masked_probs.sum() + 1e-10)\n",
    "            \n",
    "            # Sample action\n",
    "            dist = torch.distributions.Categorical(masked_probs)\n",
    "            action = dist.sample().item()\n",
    "            \n",
    "            # Safety check\n",
    "            if action not in valid_actions:\n",
    "                action = random.choice(valid_actions)\n",
    "            \n",
    "        return action, state_val.item()\n",
    "    \n",
    "    def update(self, memory):\n",
    "        \"\"\"FIXED: Update policy with proper tensor dimensions.\"\"\"\n",
    "        # Calculate discounted rewards\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        if len(rewards) > 1:\n",
    "            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "        \n",
    "        # FIXED: Proper numpy array conversion\n",
    "        old_states = torch.FloatTensor(np.array(memory.states)).to(self.device)\n",
    "        old_actions = torch.LongTensor(memory.actions).to(self.device)\n",
    "        old_logprobs = torch.FloatTensor(memory.logprobs).to(self.device)\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluate old actions and values\n",
    "            action_probs, state_values = self.policy(old_states)\n",
    "            \n",
    "            dist = torch.distributions.Categorical(action_probs)\n",
    "            new_logprobs = dist.log_prob(old_actions)\n",
    "            entropy = dist.entropy()\n",
    "            \n",
    "            # Importance ratio\n",
    "            ratio = torch.exp(new_logprobs - old_logprobs)\n",
    "            \n",
    "            # Surrogate loss\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            \n",
    "            # FIXED: Both state_values and rewards are now 1D\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * entropy\n",
    "            \n",
    "            # Gradient step with clipping\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Update old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "print(\"FIXED PPO Agent defined!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23496bf",
   "metadata": {},
   "source": [
    "#  FIXED PPO Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ad01330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXED PPO training function defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_ppo_agent(episodes=500, opponent='random'):\n",
    "    \"\"\"\n",
    "    FIXED: Train PPO agent with proper logprob calculation.\n",
    "    \"\"\"\n",
    "    env = make(\"connectx\")\n",
    "    \n",
    "    state_size = env.configuration.columns * env.configuration.rows * 3\n",
    "    action_size = env.configuration.columns\n",
    "    \n",
    "    agent = PPOAgent(state_size, action_size)\n",
    "    memory = Memory()\n",
    "    \n",
    "    wins_window = deque(maxlen=100)\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    print(\"Training PPO agent...\")\n",
    "    for episode in range(episodes):\n",
    "        # Set up opponent\n",
    "        if opponent == 'random':\n",
    "            trainer = env.train([None, random_agent])\n",
    "        elif opponent == 'rule_based':\n",
    "            trainer = env.train([None, rule_based_agent])\n",
    "        else:\n",
    "            trainer = env.train([None, opponent])\n",
    "            \n",
    "        obs = trainer.reset()\n",
    "        state = encode_board(obs, env.configuration)\n",
    "        score = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get valid actions\n",
    "            valid_actions = get_valid_actions(obs, env.configuration)\n",
    "            \n",
    "            # Select action\n",
    "            action, state_val = agent.act(state, valid_actions)\n",
    "            \n",
    "            # Get action probability for logging\n",
    "            state_tensor = torch.FloatTensor(state).to(agent.device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                action_probs, _ = agent.policy_old(state_tensor)\n",
    "                masked_probs = torch.zeros_like(action_probs)\n",
    "                for act in valid_actions:\n",
    "                    masked_probs[0][act] = action_probs[0][act]\n",
    "                masked_probs = masked_probs / (masked_probs.sum() + 1e-10)\n",
    "                action_logprob = torch.log(masked_probs[0][action] + 1e-10)\n",
    "            \n",
    "            # Store state, action, logprob\n",
    "            memory.states.append(state)\n",
    "            memory.actions.append(action)\n",
    "            memory.logprobs.append(action_logprob.item())\n",
    "            \n",
    "            # Take action\n",
    "            next_obs, reward, done, info = trainer.step(action)\n",
    "            \n",
    "            # Reward shaping\n",
    "            if done:\n",
    "                if reward == 1:\n",
    "                    shaped_reward = 1.0\n",
    "                    wins_window.append(1)\n",
    "                elif reward == -1:\n",
    "                    shaped_reward = -1.0\n",
    "                    wins_window.append(0)\n",
    "                else:\n",
    "                    shaped_reward = 0.5\n",
    "                    wins_window.append(0)\n",
    "            else:\n",
    "                shaped_reward = 0.0\n",
    "            \n",
    "            # Store reward and terminal\n",
    "            memory.rewards.append(shaped_reward)\n",
    "            memory.is_terminals.append(done)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = encode_board(next_obs, env.configuration)\n",
    "            obs = next_obs\n",
    "            score += shaped_reward\n",
    "        \n",
    "        scores_window.append(score)\n",
    "        \n",
    "        # Update policy every 10 episodes\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            agent.update(memory)\n",
    "            memory.clear_memory()\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_score = np.mean(scores_window) if scores_window else 0\n",
    "            win_rate = np.mean(wins_window) * 100 if wins_window else 0\n",
    "            print(f'Episode {episode+1}/{episodes} | Win Rate: {win_rate:.1f}% | Avg Score: {avg_score:.2f}')\n",
    "    \n",
    "    print(f'Training completed! Final win rate: {np.mean(wins_window) * 100:.1f}%')\n",
    "    return agent\n",
    "\n",
    "print(\"FIXED PPO training function defined!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e882f4e",
   "metadata": {},
   "source": [
    "#  Agent Comparison Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3ae8ffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison function defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compare_agents(agent1, agent2, agent1_name, agent2_name, n_rounds=100):\n",
    "    \"\"\"Compare performance of two agents.\"\"\"\n",
    "    print(f\"\\nComparing {agent1_name} vs {agent2_name} over {n_rounds} rounds...\")\n",
    "    \n",
    "    env = make(\"connectx\")\n",
    "    agent1_wins = 0\n",
    "    agent2_wins = 0\n",
    "    draws = 0\n",
    "    \n",
    "    for i in range(n_rounds):\n",
    "        if i % 2 == 0:\n",
    "            # agent1 plays first\n",
    "            trainer = env.train([None, agent2])\n",
    "            obs = trainer.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                my_action = agent1(obs, env.configuration)\n",
    "                obs, reward, done, info = trainer.step(my_action)\n",
    "                if not done:\n",
    "                    my_action = agent2(obs, env.configuration)\n",
    "                    obs, reward, done, info = trainer.step(my_action)\n",
    "            \n",
    "            if reward == 1:\n",
    "                agent1_wins += 1\n",
    "            elif reward == -1:\n",
    "                agent2_wins += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "        else:\n",
    "            # agent2 plays first\n",
    "            trainer = env.train([None, agent1])\n",
    "            obs = trainer.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                my_action = agent2(obs, env.configuration)\n",
    "                obs, reward, done, info = trainer.step(my_action)\n",
    "                if not done:\n",
    "                    my_action = agent1(obs, env.configuration)\n",
    "                    obs, reward, done, info = trainer.step(my_action)\n",
    "            \n",
    "            if reward == 1:\n",
    "                agent2_wins += 1\n",
    "            elif reward == -1:\n",
    "                agent1_wins += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "    \n",
    "    print(f\"{agent1_name} wins: {agent1_wins} ({agent1_wins/n_rounds*100:.1f}%)\")\n",
    "    print(f\"{agent2_name} wins: {agent2_wins} ({agent2_wins/n_rounds*100:.1f}%)\")\n",
    "    print(f\"Draws: {draws} ({draws/n_rounds*100:.1f}%)\")\n",
    "    \n",
    "    return agent1_wins, agent2_wins, draws\n",
    "\n",
    "print(\"Comparison function defined!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da9c7a",
   "metadata": {},
   "source": [
    "#  Test Baseline Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "574e4efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING BASELINE AGENTS\n",
      "============================================================\n",
      "\n",
      "Comparing Rule-based vs Random over 50 rounds...\n",
      "Rule-based wins: 46 (92.0%)\n",
      "Random wins: 4 (8.0%)\n",
      "Draws: 0 (0.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46, 4, 0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING BASELINE AGENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "compare_agents(rule_based_agent, random_agent, \"Rule-based\", \"Random\", n_rounds=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905c458c",
   "metadata": {},
   "source": [
    "#  Train DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2585852d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING DQN AGENT\n",
      "============================================================\n",
      "\n",
      "Phase 1: Training against random agent...\n",
      "Training DQN agent...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train against random agent first\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPhase 1: Training against random agent...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m dqn_agent_trained \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopponent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Optionally train more against rule-based\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPhase 2: Training against rule-based agent...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[109], line 27\u001b[0m, in \u001b[0;36mtrain_dqn_agent\u001b[1;34m(episodes, opponent, epsilon_start)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mtrain([\u001b[38;5;28;01mNone\u001b[39;00m, opponent])\n\u001b[1;32m---> 27\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m encode_board(obs, env\u001b[38;5;241m.\u001b[39mconfiguration)\n\u001b[0;32m     29\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle_environments\\core.py:438\u001b[0m, in \u001b[0;36mEnvironment.train.<locals>.reset\u001b[1;34m()\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m():\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m runner\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m     runner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__agent_runner(agents)\n\u001b[0;32m    440\u001b[0m     advance()\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle_environments\\core.py:308\u001b[0m, in \u001b[0;36mEnvironment.reset\u001b[1;34m(self, num_agents)\u001b[0m\n\u001b[0;32m    305\u001b[0m     num_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecification\u001b[38;5;241m.\u001b[39magents[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Get configuration default state.\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__set_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# Reset all agents to status=INACTIVE (copy out values to reset afterwards).\u001b[39;00m\n\u001b[0;32m    310\u001b[0m statuses \u001b[38;5;241m=\u001b[39m [a\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate]\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle_environments\\core.py:548\u001b[0m, in \u001b[0;36mEnvironment.__set_state\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecification\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidArgument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(state)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid number of agent(s).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m structify([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_state(index, s) \u001b[38;5;28;01mfor\u001b[39;00m index, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(state)])\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate]\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle_environments\\core.py:548\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecification\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidArgument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(state)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid number of agent(s).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m structify([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m index, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(state)])\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate]\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle_environments\\core.py:573\u001b[0m, in \u001b[0;36mEnvironment.__get_state\u001b[1;34m(self, position, state)\u001b[0m\n\u001b[0;32m    569\u001b[0m     props \u001b[38;5;241m=\u001b[39m structify(update_props(copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__state_schema\u001b[38;5;241m.\u001b[39mproperties)))\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__state_schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m: props})\n\u001b[1;32m--> 573\u001b[0m err, data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err:\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidArgument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault state generation failed for #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m err)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle_environments\\utils.py:171\u001b[0m, in \u001b[0;36mprocess_schema\u001b[1;34m(schema, data, use_default)\u001b[0m\n\u001b[0;32m    169\u001b[0m     data \u001b[38;5;241m=\u001b[39m default_schema(schema, deepcopy(data))\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     \u001b[43mjsonschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    173\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:1328\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(instance, schema, cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m validator_for(schema)\n\u001b[1;32m-> 1328\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m validator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(schema, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1330\u001b[0m error \u001b[38;5;241m=\u001b[39m exceptions\u001b[38;5;241m.\u001b[39mbest_match(validator\u001b[38;5;241m.\u001b[39miter_errors(instance))\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:316\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.check_schema\u001b[1;34m(cls, schema, format_checker)\u001b[0m\n\u001b[0;32m    311\u001b[0m     format_checker \u001b[38;5;241m=\u001b[39m Validator\u001b[38;5;241m.\u001b[39mFORMAT_CHECKER\n\u001b[0;32m    312\u001b[0m validator \u001b[38;5;241m=\u001b[39m Validator(\n\u001b[0;32m    313\u001b[0m     schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mMETA_SCHEMA,\n\u001b[0;32m    314\u001b[0m     format_checker\u001b[38;5;241m=\u001b[39mformat_checker,\n\u001b[0;32m    315\u001b[0m )\n\u001b[1;32m--> 316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m validator\u001b[38;5;241m.\u001b[39miter_errors(schema):\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mSchemaError\u001b[38;5;241m.\u001b[39mcreate_from(error)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:384\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.iter_errors\u001b[1;34m(self, instance, _schema)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m validator, k, v \u001b[38;5;129;01min\u001b[39;00m validators:\n\u001b[0;32m    383\u001b[0m     errors \u001b[38;5;241m=\u001b[39m validator(\u001b[38;5;28mself\u001b[39m, v, instance, _schema) \u001b[38;5;129;01mor\u001b[39;00m ()\n\u001b[1;32m--> 384\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors:\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;66;03m# set details if not already set by the called fn\u001b[39;00m\n\u001b[0;32m    386\u001b[0m         error\u001b[38;5;241m.\u001b[39m_set(\n\u001b[0;32m    387\u001b[0m             validator\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    388\u001b[0m             validator_value\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m             type_checker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTYPE_CHECKER,\n\u001b[0;32m    392\u001b[0m         )\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ref\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\_keywords.py:334\u001b[0m, in \u001b[0;36mallOf\u001b[1;34m(validator, allOf, instance, schema)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mallOf\u001b[39m(validator, allOf, instance, schema):\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, subschema \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(allOf):\n\u001b[1;32m--> 334\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mdescend(instance, subschema, schema_path\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:432\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.descend\u001b[1;34m(self, instance, schema, path, schema_path, resolver)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    431\u001b[0m errors \u001b[38;5;241m=\u001b[39m validator(evolved, v, instance, schema) \u001b[38;5;129;01mor\u001b[39;00m ()\n\u001b[1;32m--> 432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# set details if not already set by the called fn\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     error\u001b[38;5;241m.\u001b[39m_set(\n\u001b[0;32m    435\u001b[0m         validator\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    436\u001b[0m         validator_value\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m         type_checker\u001b[38;5;241m=\u001b[39mevolved\u001b[38;5;241m.\u001b[39mTYPE_CHECKER,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ref\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\_keywords.py:275\u001b[0m, in \u001b[0;36mref\u001b[1;34m(validator, ref, instance, schema)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mref\u001b[39m(validator, ref, instance, schema):\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m validator\u001b[38;5;241m.\u001b[39m_validate_reference(ref\u001b[38;5;241m=\u001b[39mref, instance\u001b[38;5;241m=\u001b[39minstance)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:432\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.descend\u001b[1;34m(self, instance, schema, path, schema_path, resolver)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    431\u001b[0m errors \u001b[38;5;241m=\u001b[39m validator(evolved, v, instance, schema) \u001b[38;5;129;01mor\u001b[39;00m ()\n\u001b[1;32m--> 432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# set details if not already set by the called fn\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     error\u001b[38;5;241m.\u001b[39m_set(\n\u001b[0;32m    435\u001b[0m         validator\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    436\u001b[0m         validator_value\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m         type_checker\u001b[38;5;241m=\u001b[39mevolved\u001b[38;5;241m.\u001b[39mTYPE_CHECKER,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ref\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\_keywords.py:296\u001b[0m, in \u001b[0;36mproperties\u001b[1;34m(validator, properties, instance, schema)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mproperty\u001b[39m, subschema \u001b[38;5;129;01min\u001b[39;00m properties\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mproperty\u001b[39m \u001b[38;5;129;01min\u001b[39;00m instance:\n\u001b[1;32m--> 296\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mdescend(\n\u001b[0;32m    297\u001b[0m             instance[\u001b[38;5;28mproperty\u001b[39m],\n\u001b[0;32m    298\u001b[0m             subschema,\n\u001b[0;32m    299\u001b[0m             path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mproperty\u001b[39m,\n\u001b[0;32m    300\u001b[0m             schema_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mproperty\u001b[39m,\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:432\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.descend\u001b[1;34m(self, instance, schema, path, schema_path, resolver)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    431\u001b[0m errors \u001b[38;5;241m=\u001b[39m validator(evolved, v, instance, schema) \u001b[38;5;129;01mor\u001b[39;00m ()\n\u001b[1;32m--> 432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# set details if not already set by the called fn\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     error\u001b[38;5;241m.\u001b[39m_set(\n\u001b[0;32m    435\u001b[0m         validator\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    436\u001b[0m         validator_value\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m         type_checker\u001b[38;5;241m=\u001b[39mevolved\u001b[38;5;241m.\u001b[39mTYPE_CHECKER,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ref\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\_keywords.py:44\u001b[0m, in \u001b[0;36madditionalProperties\u001b[1;34m(validator, aP, instance, schema)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mis_type(aP, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m extra \u001b[38;5;129;01min\u001b[39;00m extras:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mdescend(instance[extra], aP, path\u001b[38;5;241m=\u001b[39mextra)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m aP \u001b[38;5;129;01mand\u001b[39;00m extras:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatternProperties\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m schema:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:432\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.descend\u001b[1;34m(self, instance, schema, path, schema_path, resolver)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    431\u001b[0m errors \u001b[38;5;241m=\u001b[39m validator(evolved, v, instance, schema) \u001b[38;5;129;01mor\u001b[39;00m ()\n\u001b[1;32m--> 432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# set details if not already set by the called fn\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     error\u001b[38;5;241m.\u001b[39m_set(\n\u001b[0;32m    435\u001b[0m         validator\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    436\u001b[0m         validator_value\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m         type_checker\u001b[38;5;241m=\u001b[39mevolved\u001b[38;5;241m.\u001b[39mTYPE_CHECKER,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ref\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\_keywords.py:279\u001b[0m, in \u001b[0;36mdynamicRef\u001b[1;34m(validator, dynamicRef, instance, schema)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdynamicRef\u001b[39m(validator, dynamicRef, instance, schema):\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m validator\u001b[38;5;241m.\u001b[39m_validate_reference(ref\u001b[38;5;241m=\u001b[39mdynamicRef, instance\u001b[38;5;241m=\u001b[39minstance)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:432\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.descend\u001b[1;34m(self, instance, schema, path, schema_path, resolver)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    431\u001b[0m errors \u001b[38;5;241m=\u001b[39m validator(evolved, v, instance, schema) \u001b[38;5;129;01mor\u001b[39;00m ()\n\u001b[1;32m--> 432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# set details if not already set by the called fn\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     error\u001b[38;5;241m.\u001b[39m_set(\n\u001b[0;32m    435\u001b[0m         validator\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    436\u001b[0m         validator_value\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m         type_checker\u001b[38;5;241m=\u001b[39mevolved\u001b[38;5;241m.\u001b[39mTYPE_CHECKER,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ref\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\_keywords.py:334\u001b[0m, in \u001b[0;36mallOf\u001b[1;34m(validator, allOf, instance, schema)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mallOf\u001b[39m(validator, allOf, instance, schema):\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, subschema \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(allOf):\n\u001b[1;32m--> 334\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mdescend(instance, subschema, schema_path\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:432\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.descend\u001b[1;34m(self, instance, schema, path, schema_path, resolver)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    431\u001b[0m errors \u001b[38;5;241m=\u001b[39m validator(evolved, v, instance, schema) \u001b[38;5;129;01mor\u001b[39;00m ()\n\u001b[1;32m--> 432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# set details if not already set by the called fn\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     error\u001b[38;5;241m.\u001b[39m_set(\n\u001b[0;32m    435\u001b[0m         validator\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    436\u001b[0m         validator_value\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m         type_checker\u001b[38;5;241m=\u001b[39mevolved\u001b[38;5;241m.\u001b[39mTYPE_CHECKER,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ref\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\_keywords.py:275\u001b[0m, in \u001b[0;36mref\u001b[1;34m(validator, ref, instance, schema)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mref\u001b[39m(validator, ref, instance, schema):\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m validator\u001b[38;5;241m.\u001b[39m_validate_reference(ref\u001b[38;5;241m=\u001b[39mref, instance\u001b[38;5;241m=\u001b[39minstance)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:432\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.descend\u001b[1;34m(self, instance, schema, path, schema_path, resolver)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    431\u001b[0m errors \u001b[38;5;241m=\u001b[39m validator(evolved, v, instance, schema) \u001b[38;5;129;01mor\u001b[39;00m ()\n\u001b[1;32m--> 432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# set details if not already set by the called fn\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     error\u001b[38;5;241m.\u001b[39m_set(\n\u001b[0;32m    435\u001b[0m         validator\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    436\u001b[0m         validator_value\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m         type_checker\u001b[38;5;241m=\u001b[39mevolved\u001b[38;5;241m.\u001b[39mTYPE_CHECKER,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ref\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\_keywords.py:296\u001b[0m, in \u001b[0;36mproperties\u001b[1;34m(validator, properties, instance, schema)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mproperty\u001b[39m, subschema \u001b[38;5;129;01min\u001b[39;00m properties\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mproperty\u001b[39m \u001b[38;5;129;01min\u001b[39;00m instance:\n\u001b[1;32m--> 296\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mdescend(\n\u001b[0;32m    297\u001b[0m             instance[\u001b[38;5;28mproperty\u001b[39m],\n\u001b[0;32m    298\u001b[0m             subschema,\n\u001b[0;32m    299\u001b[0m             path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mproperty\u001b[39m,\n\u001b[0;32m    300\u001b[0m             schema_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mproperty\u001b[39m,\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:432\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.descend\u001b[1;34m(self, instance, schema, path, schema_path, resolver)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    431\u001b[0m errors \u001b[38;5;241m=\u001b[39m validator(evolved, v, instance, schema) \u001b[38;5;129;01mor\u001b[39;00m ()\n\u001b[1;32m--> 432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# set details if not already set by the called fn\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     error\u001b[38;5;241m.\u001b[39m_set(\n\u001b[0;32m    435\u001b[0m         validator\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    436\u001b[0m         validator_value\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m         type_checker\u001b[38;5;241m=\u001b[39mevolved\u001b[38;5;241m.\u001b[39mTYPE_CHECKER,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ref\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\_keywords.py:279\u001b[0m, in \u001b[0;36mdynamicRef\u001b[1;34m(validator, dynamicRef, instance, schema)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdynamicRef\u001b[39m(validator, dynamicRef, instance, schema):\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m validator\u001b[38;5;241m.\u001b[39m_validate_reference(ref\u001b[38;5;241m=\u001b[39mdynamicRef, instance\u001b[38;5;241m=\u001b[39minstance)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:432\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.descend\u001b[1;34m(self, instance, schema, path, schema_path, resolver)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    431\u001b[0m errors \u001b[38;5;241m=\u001b[39m validator(evolved, v, instance, schema) \u001b[38;5;129;01mor\u001b[39;00m ()\n\u001b[1;32m--> 432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;66;03m# set details if not already set by the called fn\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     error\u001b[38;5;241m.\u001b[39m_set(\n\u001b[0;32m    435\u001b[0m         validator\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    436\u001b[0m         validator_value\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m         type_checker\u001b[38;5;241m=\u001b[39mevolved\u001b[38;5;241m.\u001b[39mTYPE_CHECKER,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$ref\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\_keywords.py:334\u001b[0m, in \u001b[0;36mallOf\u001b[1;34m(validator, allOf, instance, schema)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mallOf\u001b[39m(validator, allOf, instance, schema):\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, subschema \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(allOf):\n\u001b[1;32m--> 334\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mdescend(instance, subschema, schema_path\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:424\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.descend\u001b[1;34m(self, instance, schema, path, schema_path, resolver)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m         resolver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolver\u001b[38;5;241m.\u001b[39min_subresource(\n\u001b[0;32m    422\u001b[0m             specification\u001b[38;5;241m.\u001b[39mcreate_resource(schema),\n\u001b[0;32m    423\u001b[0m         )\n\u001b[1;32m--> 424\u001b[0m     evolved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_resolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m applicable_validators(schema):\n\u001b[0;32m    427\u001b[0m     validator \u001b[38;5;241m=\u001b[39m evolved\u001b[38;5;241m.\u001b[39mVALIDATORS\u001b[38;5;241m.\u001b[39mget(k)\n",
      "File \u001b[1;32mc:\\Users\\tisha_madame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jsonschema\\validators.py:-1\u001b[0m, in \u001b[0;36mcreate.<locals>.Validator.evolve\u001b[1;34m(self, **changes)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING DQN AGENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train against random agent first\n",
    "print(\"\\nPhase 1: Training against random agent...\")\n",
    "dqn_agent_trained = train_dqn_agent(episodes=500, opponent='random')\n",
    "\n",
    "# Optionally train more against rule-based\n",
    "print(\"\\nPhase 2: Training against rule-based agent...\")\n",
    "dqn_agent_final = train_dqn_agent(episodes=500, opponent='rule_based', epsilon_start=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a35e92",
   "metadata": {},
   "source": [
    "#  Create DQN Wrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dqn_agent_wrapper(obs, config, trained_agent=dqn_agent_final):\n",
    "    \"\"\"Wrapper for trained DQN agent.\"\"\"\n",
    "    state = encode_board(obs, config)\n",
    "    valid_actions = get_valid_actions(obs, config)\n",
    "    action = trained_agent.act(state, valid_actions, eps=0.0)\n",
    "    return action\n",
    "\n",
    "print(\"DQN wrapper created!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c8a908",
   "metadata": {},
   "source": [
    "#  Evaluate DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING DQN AGENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "compare_agents(dqn_agent_wrapper, random_agent, \"DQN\", \"Random\", n_rounds=100)\n",
    "compare_agents(dqn_agent_wrapper, rule_based_agent, \"DQN\", \"Rule-based\", n_rounds=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa40d6",
   "metadata": {},
   "source": [
    "#  Train PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d85b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING PPO AGENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPhase 1: Training against random agent...\")\n",
    "ppo_agent_trained = train_ppo_agent(episodes=300, opponent='random')\n",
    "\n",
    "print(\"\\nPhase 2: Training against rule-based agent...\")\n",
    "ppo_agent_final = train_ppo_agent(episodes=300, opponent='rule_based')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a8d00",
   "metadata": {},
   "source": [
    "#  Create PPO Wrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ppo_agent_wrapper(obs, config, trained_agent=ppo_agent_final):\n",
    "    \"\"\"Wrapper for trained PPO agent.\"\"\"\n",
    "    state = encode_board(obs, config)\n",
    "    valid_actions = get_valid_actions(obs, config)\n",
    "    action, _ = trained_agent.act(state, valid_actions)\n",
    "    return action\n",
    "\n",
    "print(\"PPO wrapper created!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186efd57",
   "metadata": {},
   "source": [
    "#  Evaluate PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING PPO AGENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "compare_agents(ppo_agent_wrapper, random_agent, \"PPO\", \"Random\", n_rounds=100)\n",
    "compare_agents(ppo_agent_wrapper, rule_based_agent, \"PPO\", \"Rule-based\", n_rounds=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded5f65",
   "metadata": {},
   "source": [
    "#  Compare DQN vs PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DQN vs PPO COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "compare_agents(dqn_agent_wrapper, ppo_agent_wrapper, \"DQN\", \"PPO\", n_rounds=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334e42c",
   "metadata": {},
   "source": [
    "#  Visualize Sample Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa9c6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualize_game(agent1, agent2, agent1_name=\"Agent 1\", agent2_name=\"Agent 2\"):\n",
    "    \"\"\"Visualize a sample game between two agents.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GAME: {agent1_name} vs {agent2_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    env = make(\"connectx\")\n",
    "    trainer = env.train([None, agent2])\n",
    "    obs = trainer.reset()\n",
    "    \n",
    "    print(\"\\nInitial board:\")\n",
    "    board = np.array(obs.board).reshape(env.configuration.rows, env.configuration.columns)\n",
    "    print(board)\n",
    "    \n",
    "    step = 1\n",
    "    done = False\n",
    "    while not done:\n",
    "        # agent1's turn\n",
    "        action = agent1(obs, env.configuration)\n",
    "        print(f\"\\nStep {step}: {agent1_name} â†’ column {action}\")\n",
    "        obs, reward, done, info = trainer.step(action)\n",
    "        \n",
    "        if not done:\n",
    "            # agent2's turn\n",
    "            action = agent2(obs, env.configuration)\n",
    "            print(f\"Step {step}: {agent2_name} â†’ column {action}\")\n",
    "            obs, reward, done, info = trainer.step(action)\n",
    "        \n",
    "        # Show board\n",
    "        board = np.array(obs.board).reshape(env.configuration.rows, env.configuration.columns)\n",
    "        print(board)\n",
    "        \n",
    "        step += 1\n",
    "        if step > 50:\n",
    "            print(\"\\nGame exceeded 50 steps!\")\n",
    "            break\n",
    "    \n",
    "    # Result\n",
    "    if reward == 1:\n",
    "        winner = agent1_name\n",
    "    elif reward == -1:\n",
    "        winner = agent2_name\n",
    "    else:\n",
    "        winner = \"Draw\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULT: {winner} wins!\")\n",
    "    print('='*60)\n",
    "\n",
    "# Visualize sample games\n",
    "visualize_game(rule_based_agent, random_agent, \"Rule-based\", \"Random\")\n",
    "visualize_game(dqn_agent_wrapper, rule_based_agent, \"DQN\", \"Rule-based\")\n",
    "visualize_game(ppo_agent_wrapper, rule_based_agent, \"PPO\", \"Rule-based\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1dd3c",
   "metadata": {},
   "source": [
    "#  Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef43d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nAll tests completed!\")\n",
    "print(\"\\nKey improvements in this fixed version:\")\n",
    "print(\"1. âœ“ DQN agent properly masks invalid actions\")\n",
    "print(\"2. âœ“ Target network updates at correct frequency\")\n",
    "print(\"3. âœ“ Reward shaping is balanced (Â±1 for win/loss)\")\n",
    "print(\"4. âœ“ PPO critic outputs 1D tensors (no dimension mismatch)\")\n",
    "print(\"5. âœ“ Proper win rate and reward tracking\")\n",
    "print(\"6. âœ“ Gradient clipping prevents exploding gradients\")\n",
    "print(\"7. âœ“ Agents should now learn effectively!\")\n",
    "\n",
    "print(\"\\nExpected performance after training:\")\n",
    "print(\"- DQN vs Random: 60-80% win rate\")\n",
    "print(\"- DQN vs Rule-based: 30-50% win rate\")\n",
    "print(\"- PPO vs Random: 70-90% win rate\")\n",
    "print(\"- PPO vs Rule-based: 40-60% win rate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
